{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94bd5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, LeaveOneOut\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris, make_regression\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "# Cross-validation in machine learning\n",
    "# Itâ€™s a technique to check how well a model performs on new data.\n",
    "# Instead of just splitting the dataset into training and test sets once, cross-validation tests the model multiple times on different subsets.\n",
    "# Common approaches include:\n",
    "# - Holdout Method: Splitting the dataset into training and test sets (e.g., 80%-20%)\n",
    "# - K-Fold Cross-Validation: Dividing data into K parts, training on K-1 parts, and testing on the remaining part\n",
    "# - Stratified K-Fold: Similar to K-Fold but ensures class proportions remain balanced\n",
    "# - LOOCV (Leave-One-Out Cross Validation): Uses every single data point as a test set one at a time\n",
    "# - Time Series Cross-Validation: Used for time-dependent data, ensuring past data is used to predict future data\n",
    "\n",
    "def cross_validation_methods():\n",
    "    \"\"\" Demonstrates different cross-validation techniques \"\"\"\n",
    "    X, y = load_iris(return_X_y=True)\n",
    "    model = DecisionTreeClassifier()\n",
    "    \n",
    "    print(\"Holdout Cross Validation:\")\n",
    "    scores = cross_val_score(model, X, y, cv=5)\n",
    "    print(\"Mean Accuracy:\", np.mean(scores))\n",
    "    \n",
    "    print(\"\\nK-Fold Cross Validation:\")\n",
    "    kf = KFold(n_splits=5)\n",
    "    scores = cross_val_score(model, X, y, cv=kf)\n",
    "    print(\"Mean Accuracy:\", np.mean(scores))\n",
    "    \n",
    "    print(\"\\nStratified K-Fold Cross Validation:\")\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    scores = cross_val_score(model, X, y, cv=skf)\n",
    "    print(\"Mean Accuracy:\", np.mean(scores))\n",
    "    \n",
    "    print(\"\\nLeave-One-Out Cross Validation:\")\n",
    "    loo = LeaveOneOut()\n",
    "    scores = cross_val_score(model, X, y, cv=loo)\n",
    "    print(\"Mean Accuracy:\", np.mean(scores))\n",
    "\n",
    "# Assumptions of Linear Regression\n",
    "# - Linearity: The relationship between input and output should be a straight line\n",
    "# - Independence: Data points should not be dependent on each other\n",
    "# - Homoscedasticity: The spread of errors should be consistent across values\n",
    "# - Normality of Residuals: Errors should follow a normal distribution\n",
    "# - No Multicollinearity: Independent variables should not be highly correlated\n",
    "\n",
    "def linear_regression_assumptions():\n",
    "    \"\"\" Checks assumptions of Linear Regression \"\"\"\n",
    "    X, y = make_regression(n_samples=100, n_features=1, noise=10)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    predictions = model.predict(X)\n",
    "    \n",
    "    # Plot Residuals\n",
    "    residuals = y - predictions\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.scatterplot(x=predictions, y=residuals)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel(\"Predicted Values\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.title(\"Residual Plot to Check Homoscedasticity\")\n",
    "    plt.show()\n",
    "\n",
    "# Machine Learning Models\n",
    "# - Decision Tree: A simple, rule-based model that splits data based on conditions\n",
    "# - Random Forest: A collection of decision trees improving accuracy and reducing overfitting\n",
    "# - SVM (Support Vector Machine): Finds the best boundary between categories, useful for high-dimensional data\n",
    "\n",
    "def compare_models():\n",
    "    \"\"\" Compares Decision Tree, Random Forest, and SVM on the Iris dataset \"\"\"\n",
    "    X, y = load_iris(return_X_y=True)\n",
    "    \n",
    "    models = {\n",
    "        \"Decision Tree\": DecisionTreeClassifier(),\n",
    "        \"Random Forest\": RandomForestClassifier(),\n",
    "        \"SVM\": SVC()\n",
    "    }\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        model.fit(X, y)\n",
    "        predictions = model.predict(X)\n",
    "        acc = accuracy_score(y, predictions)\n",
    "        print(f\"{name} Accuracy: {acc:.2f}\")\n",
    "\n",
    "# Run the functions\n",
    "cross_validation_methods()\n",
    "linear_regression_assumptions()\n",
    "compare_models()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
